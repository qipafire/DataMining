{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec6cadda",
   "metadata": {},
   "source": [
    "# 互评作业2: 频繁模式挖掘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8243a275",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c27c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_dir = \"./data/30G_data\"\n",
    "output_dir = \"./data/30G_data\"\n",
    "file_count = 16\n",
    "\n",
    "for i in range(file_count):\n",
    "    input_filename = f\"part-{i:05d}.parquet\"\n",
    "    output_filename = f\"part-{i:02d}.parquet\"\n",
    "\n",
    "    input_path = os.path.join(input_dir, input_filename)\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "    print(f\"\\n正在处理文件: {input_filename}\")\n",
    "\n",
    "    df = pd.read_parquet(input_path, columns=['id', 'user_name', 'fullname', 'purchase_history'])\n",
    "    processed_rows = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"处理 {input_filename}\"):\n",
    "        ph = row['purchase_history']\n",
    "\n",
    "        if isinstance(ph, str):\n",
    "            try:\n",
    "                ph = json.loads(ph)\n",
    "            except json.JSONDecodeError:\n",
    "                ph = ast.literal_eval(ph)\n",
    "\n",
    "        items = ph.get('items', [])\n",
    "        items_id = [item.get('id') for item in items if isinstance(item, dict)]\n",
    "        new_row = {\n",
    "            'id': row['id'],\n",
    "            'user_name': row['user_name'],\n",
    "            'fullname': row['fullname'],\n",
    "            'items': items_id,\n",
    "            'payment_method': ph.get('payment_method'),\n",
    "            'payment_status': ph.get('payment_status'),\n",
    "            'purchase_date': ph.get('purchase_date')\n",
    "        }\n",
    "\n",
    "        processed_rows.append(new_row)\n",
    "\n",
    "    new_df = pd.DataFrame(processed_rows)\n",
    "    new_df.to_parquet(output_path, index=False)\n",
    "    print(f\"写入完成：{output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a891f1",
   "metadata": {},
   "source": [
    "## 商品类别关联规则挖掘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb0c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "\n",
    "data_dir = './data/30G_data'\n",
    "product_catalog_path = './data/product_catalog.json'\n",
    "categories_path = './data/categories.json'\n",
    "output_dir = './output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(product_catalog_path, 'r', encoding='utf-8') as f:\n",
    "    product_catalog = json.load(f)\n",
    "item2category2 = {item['id']: item['category'] for item in product_catalog}\n",
    "\n",
    "with open(categories_path, 'r', encoding='utf-8') as f:\n",
    "    category_hierarchy = json.load(f)\n",
    "\n",
    "category2_to_category1 = {}\n",
    "for cat1, cat2_list in category_hierarchy.items():\n",
    "    for cat2 in cat2_list:\n",
    "        category2_to_category1[cat2] = cat1\n",
    "\n",
    "files = [f'part-{i:02d}.parquet' for i in range(16)]\n",
    "batches = [files[i:i + 4] for i in range(0, len(files), 4)]\n",
    "\n",
    "for batch_id, batch_files in enumerate(batches):\n",
    "    print(f\"\\n正在处理第 {batch_id + 1} 批文件：{batch_files}\")\n",
    "    transactions = []\n",
    "\n",
    "    for fname in tqdm(batch_files, desc=\"读取事务\"):\n",
    "        path = os.path.join(data_dir, fname)\n",
    "        df = pd.read_parquet(path, columns=[\"items\"])\n",
    "\n",
    "        for items in df['items']:\n",
    "            try:\n",
    "                categories = []\n",
    "                for item in items:\n",
    "                    cat2 = item2category2.get(item)\n",
    "                    cat1 = category2_to_category1.get(cat2)\n",
    "                    if cat1:\n",
    "                        categories.append(cat1)\n",
    "                if categories:\n",
    "                    transactions.append(list(set(categories)))  # 去重\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    te = TransactionEncoder()\n",
    "    te_array = te.fit(transactions).transform(transactions)\n",
    "    df_trans = pd.DataFrame(te_array, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = fpgrowth(df_trans, min_support=0.02, use_colnames=True)\n",
    "    frequent_itemsets.to_csv(f\"{output_dir}/frequent_itemsets_{batch_id}.csv\", index=False)\n",
    "\n",
    "    print(f\"批次 {batch_id + 1} 完成，共 {len(frequent_itemsets)} 个频繁项集\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b70f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "def parse_frozenset_string(s):\n",
    "    if s.startswith(\"frozenset(\") and s.endswith(\")\"):\n",
    "        s = s[len(\"frozenset(\"):-1]\n",
    "    return frozenset(ast.literal_eval(s))\n",
    "\n",
    "all_fi = []\n",
    "\n",
    "for f in os.listdir(output_dir):\n",
    "    if f.startswith(\"frequent_itemsets_\") and f.endswith(\".csv\"):\n",
    "        fi = pd.read_csv(os.path.join(output_dir, f))\n",
    "        fi['itemsets'] = fi['itemsets'].apply(parse_frozenset_string)\n",
    "        all_fi.append(fi)\n",
    "\n",
    "all_fi_df = pd.concat(all_fi, ignore_index=True).drop_duplicates()\n",
    "print(f\"共 {len(all_fi_df)} 个去重后的频繁项集\")\n",
    "\n",
    "rules = association_rules(all_fi_df, metric=\"confidence\", min_threshold=0.5)\n",
    "\n",
    "rules.to_csv(os.path.join(output_dir, \"final_rules_category.csv\"), index=False)\n",
    "print(\"关联规则已保存到 final_rules_category.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a089e62e",
   "metadata": {},
   "source": [
    "## 支付方式与商品类别的关联分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbdede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_dir = \"./data/30G_data\"\n",
    "catalog_path = \"./data/product_catalog.json\"\n",
    "\n",
    "with open(catalog_path, 'r', encoding='utf-8') as f:\n",
    "    product_catalog = json.load(f)\n",
    "\n",
    "id2cat = {str(p[\"id\"]): p[\"category\"] for p in product_catalog}\n",
    "id2price = {str(p[\"id\"]): p[\"price\"] for p in product_catalog}\n",
    "\n",
    "transactions = []\n",
    "high_value_pm = []\n",
    "\n",
    "for i in range(16):\n",
    "    file = os.path.join(data_dir, f\"part-{i:02d}.parquet\")\n",
    "    df = pd.read_parquet(file, columns=[\"items\", \"payment_method\"])\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), desc=f\"处理文件 {file}\", total=len(df)):\n",
    "        payment_method = row['payment_method']\n",
    "        items = row['items']\n",
    "        categories = set()\n",
    "        is_high_value = False\n",
    "        for item in items:\n",
    "            item_id = str(item)\n",
    "            category = id2cat.get(item_id)\n",
    "            price = id2price.get(item_id)\n",
    "            if category:\n",
    "                categories.add(category)\n",
    "            if price and price > 5000:\n",
    "                is_high_value = True\n",
    "\n",
    "        if categories:\n",
    "            transactions.append(list(categories) + [f\"支付:{payment_method}\"])\n",
    "\n",
    "        if is_high_value:\n",
    "            high_value_pm.append(payment_method)\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "frequent_itemsets = fpgrowth(df_encoded, min_support=0.01, use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.6)\n",
    "rules_pm2cat = rules[\n",
    "    rules['antecedents'].apply(lambda x: any(str(a).startswith(\"支付:\") for a in x)) &\n",
    "    rules['consequents'].apply(lambda x: all(not str(c).startswith(\"支付:\") for c in x))\n",
    "]\n",
    "\n",
    "rules_pm2cat.to_csv(\"output/payment_category_rules.csv\", index=False)\n",
    "\n",
    "high_value_pm_df = pd.Series(high_value_pm, name=\"payment_method\").value_counts(normalize=True)\n",
    "high_value_pm_df.to_csv(\"output/high_value_payment_distribution.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9919b48b",
   "metadata": {},
   "source": [
    "## 时间序列模式挖掘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd35aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "data_dir = './data/30G_data'\n",
    "output_dir = './output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "files = [f'part-{i:02d}.parquet' for i in range(16)]\n",
    "product_catalog_path = './data/product_catalog.json'\n",
    "\n",
    "with open(product_catalog_path, 'r', encoding='utf-8') as f:\n",
    "    catalog = json.load(f)\n",
    "item2category = {int(item['id']): item['category'] for item in catalog}\n",
    "\n",
    "monthly_stats = Counter()\n",
    "quarterly_stats = Counter()\n",
    "weekday_stats = Counter()\n",
    "sequence_counter = Counter()\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    print(f\"正在处理文件：{file_path}\")\n",
    "    df = pd.read_parquet(file_path, engine='pyarrow', columns=['id', 'items', 'purchase_date'])\n",
    "\n",
    "    def parse_items(items_str):\n",
    "        try:\n",
    "            if isinstance(items_str, str):\n",
    "                arr = ast.literal_eval(items_str.replace('array', ''))\n",
    "                return [int(i) for i in arr]\n",
    "            elif isinstance(items_str, list):\n",
    "                return items_str\n",
    "            return []\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "    df['items'] = df['items'].apply(parse_items)\n",
    "    df['purchase_date'] = pd.to_datetime(df['purchase_date'], errors='coerce')\n",
    "    df.dropna(subset=['purchase_date'], inplace=True)\n",
    "    df['month'] = df['purchase_date'].dt.month\n",
    "    df['quarter'] = df['purchase_date'].dt.to_period('Q').astype(str)\n",
    "    df['weekday'] = df['purchase_date'].dt.day_name()\n",
    "\n",
    "    def map_categories(item_list):\n",
    "        return list({item2category.get(i) for i in item_list if i in item2category})\n",
    "\n",
    "    df['categories'] = df['items'].apply(map_categories)\n",
    "    for _, row in df.iterrows():\n",
    "        for cat in row['categories']:\n",
    "            monthly_stats[(row['month'], cat)] += 1\n",
    "            quarterly_stats[(row['quarter'], cat)] += 1\n",
    "            weekday_stats[(row['weekday'], cat)] += 1\n",
    "\n",
    "    df.sort_values(by=['id', 'purchase_date'], inplace=True)\n",
    "    grouped = df.groupby('id')\n",
    "\n",
    "    for _, group in grouped:\n",
    "        group = group.sort_values('purchase_date')\n",
    "        last_cats = None\n",
    "        for _, row in group.iterrows():\n",
    "            curr_cats = set(row['categories'])\n",
    "            if last_cats:\n",
    "                for a in last_cats:\n",
    "                    for b in curr_cats:\n",
    "                        if a != b:\n",
    "                            sequence_counter[(a, b)] += 1\n",
    "            last_cats = curr_cats\n",
    "\n",
    "def save_counter_to_csv(counter, columns, filename):\n",
    "    df_out = pd.DataFrame([(*k, v) for k, v in counter.items()], columns=columns)\n",
    "    df_out.to_csv(os.path.join(output_dir, filename), index=False)\n",
    "    print(f\"写入 {filename} 成功，共 {len(df_out)} 条\")\n",
    "\n",
    "save_counter_to_csv(monthly_stats, ['month', 'category', 'count'], 'monthly_stats.csv')\n",
    "save_counter_to_csv(quarterly_stats, ['quarter', 'category', 'count'], 'quarterly_stats.csv')\n",
    "save_counter_to_csv(weekday_stats, ['weekday', 'category', 'count'], 'weekday_stats.csv')\n",
    "save_counter_to_csv(sequence_counter, ['category_A', 'category_B', 'count'], 'sequence_patterns.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f069bfd",
   "metadata": {},
   "source": [
    "## 退款模式分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55d7d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_dir = './data/30G_data'\n",
    "output_dir = './output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "files = [f'part-{i:02d}.parquet' for i in range(16)]\n",
    "\n",
    "with open('./data/product_catalog.json', 'r', encoding='utf-8') as f:\n",
    "    catalog = json.load(f)\n",
    "item2category = {int(item['id']): item['category'] for item in catalog}\n",
    "\n",
    "def parse_items(items_val):\n",
    "    try:\n",
    "        if isinstance(items_val, str):\n",
    "            arr = ast.literal_eval(items_val.replace('array', ''))\n",
    "            return list(set(item2category.get(i) for i in arr if i in item2category))\n",
    "        elif isinstance(items_val, (list, tuple)):\n",
    "            return list(set(item2category.get(i) for i in items_val if i in item2category))\n",
    "        else:\n",
    "            return []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def one_hot_encode(categories_list, all_categories):\n",
    "    row = {cat: (cat in categories_list) for cat in all_categories}\n",
    "    return pd.Series(row)\n",
    "\n",
    "all_categories = set(item2category.values())\n",
    "all_rules = []\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    print(f\"处理文件: {file_path}\")\n",
    "\n",
    "    df = pd.read_parquet(file_path, engine='pyarrow', columns=['payment_status', 'items'])\n",
    "    df['categories'] = df['items'].apply(parse_items)\n",
    "\n",
    "    df_refund = df[df['payment_status'].isin(['已退款', '部分退款'])].copy()\n",
    "    if df_refund.empty:\n",
    "        print(f\"文件 {file} 中无退款订单，跳过。\")\n",
    "        continue\n",
    "\n",
    "    ohe_df = pd.DataFrame([one_hot_encode(cats, all_categories) for cats in df_refund['categories']])\n",
    "    frequent_itemsets = fpgrowth(ohe_df, min_support=0.005, use_colnames=True)\n",
    "\n",
    "    if frequent_itemsets.empty:\n",
    "        print(f\"文件 {file} 无满足支持度的频繁项集。\")\n",
    "        continue\n",
    "\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.4)\n",
    "\n",
    "    if rules.empty:\n",
    "        print(f\"文件 {file} 无满足置信度的关联规则。\")\n",
    "        continue\n",
    "\n",
    "    out_file = os.path.join(output_dir, f\"refund_rules_{file.replace('.parquet', '')}.csv\")\n",
    "    rules.to_csv(out_file, index=False)\n",
    "    print(f\"规则保存到 {out_file}，共 {len(rules)} 条规则。\")\n",
    "\n",
    "    all_rules.append(rules)\n",
    "\n",
    "if all_rules:\n",
    "    combined_rules = pd.concat(all_rules, ignore_index=True).drop_duplicates()\n",
    "    combined_rules.to_csv(os.path.join(output_dir, 'refund_rules_all.csv'), index=False)\n",
    "    print(f\"合并所有规则完成，保存到 refund_rules_all.csv ，共 {len(combined_rules)} 条规则。\")\n",
    "else:\n",
    "    print(\"未发现任何符合条件的退款关联规则。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
